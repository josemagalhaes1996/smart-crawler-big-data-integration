# Smart Crawler for Big Data integration

This project aims to produce a set of tools, that will help big data integration engineers, model the data automatically with a certain confidence interval.

## General Architecture

<p align="center">
  <img src="/img/DataGovernanceArchitecture3.png">
</p>


## Getting Started

These instructions help you start developing and running the project for testing purposes.

### Prerequisites

1. Netbeans
2. Hadoop Cluster

### Installing

To start developing

1. Install Netbeans;
2. Clone the git repository;
3. Configure as Maven project;

## Deployment

Maven package and deploy them in the Big Data infrastructure.
Change endpoints in ```AtlasClient.AtlasCosumer``` 

## Running the tests





## Built With

* [Spark](https://spark.apache.org) - The scalabe event processing engine
* [Atlas](https://atlas.apache.org/) - Data Governance and Metadata framework for Hadoop
* [Ranger](https://ranger.apache.org/) - Enable, monitor and manage comprehensive data security across the Hadoop platform.

## Authors

* **José Magalhães** 
* **João Galvão**  
* **Maria Inês Costa** 

## Versioning

We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://gitlab.com/lid4_uminho/smart-crawler-big-data-integration/-/tags). 

## License

This project is currently internal.

## Acknowledgments

* Cheers for the LID4 community 


